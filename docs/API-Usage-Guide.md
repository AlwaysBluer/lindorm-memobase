# LindormMemobase API ä½¿ç”¨æŒ‡å—

## ç›®å½•
- [ç®€ä»‹](#ç®€ä»‹)
- [å®‰è£…ä¸é…ç½®](#å®‰è£…ä¸é…ç½®)
- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [API å‚è€ƒ](#api-å‚è€ƒ)
  - [åˆå§‹åŒ–](#åˆå§‹åŒ–)
  - [å†…å­˜æå–](#å†…å­˜æå–)
  - [ç”¨æˆ·æ¡£æ¡ˆç®¡ç†](#ç”¨æˆ·æ¡£æ¡ˆç®¡ç†)
  - [äº‹ä»¶ç®¡ç†](#äº‹ä»¶ç®¡ç†)
  - [ä¸Šä¸‹æ–‡ç”Ÿæˆ](#ä¸Šä¸‹æ–‡ç”Ÿæˆ)
  - [ç¼“å†²åŒºç®¡ç†](#ç¼“å†²åŒºç®¡ç†)
- [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)
- [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)

## ç®€ä»‹

LindormMemobase æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è®°å¿†æå–å’Œç”¨æˆ·æ¡£æ¡ˆç®¡ç†ç³»ç»Ÿï¼Œä¸“ä¸º LLM åº”ç”¨è®¾è®¡ã€‚å®ƒæä¾›äº†ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

- **è®°å¿†æå–**ï¼šä»å¯¹è¯ä¸­è‡ªåŠ¨æå–å’Œç»“æ„åŒ–ç”¨æˆ·ä¿¡æ¯
- **ç”¨æˆ·æ¡£æ¡ˆç®¡ç†**ï¼šç»´æŠ¤å’Œæ›´æ–°ç”¨æˆ·çš„é•¿æœŸè®°å¿†æ¡£æ¡ˆ
- **è¯­ä¹‰æœç´¢**ï¼šåŸºäºå‘é‡åµŒå…¥çš„ç›¸ä¼¼åº¦æœç´¢
- **ä¸Šä¸‹æ–‡ç”Ÿæˆ**ï¼šä¸ºå¯¹è¯ç”Ÿæˆç›¸å…³çš„å†å²ä¸Šä¸‹æ–‡
- **ç¼“å†²åŒºç®¡ç†**ï¼šæ™ºèƒ½çš„æ•°æ®ç¼“å†²å’Œæ‰¹é‡å¤„ç†æœºåˆ¶

## å®‰è£…ä¸é…ç½®

### å®‰è£…

```bash
pip install lindormmemobase
```

### é…ç½®æ–¹å¼

LindormMemobase é‡‡ç”¨åˆ†å±‚é…ç½®ç­–ç•¥ï¼Œä¼˜å…ˆçº§ä»é«˜åˆ°ä½ä¸ºï¼š
1. **ç¯å¢ƒå˜é‡**ï¼ˆè¦†ç›–æ‰€æœ‰å…¶ä»–é…ç½®ï¼‰
2. **config.yaml**ï¼ˆé¡¹ç›®çº§é…ç½®ï¼‰
3. **é»˜è®¤å€¼**ï¼ˆå†…ç½®é»˜è®¤é…ç½®ï¼‰

æ¨èå®è·µï¼š
- **æ•æ„Ÿä¿¡æ¯**ï¼ˆAPIå¯†é’¥ã€å¯†ç ï¼‰æ”¾åœ¨ `.env` æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­
- **é¡¹ç›®é…ç½®**ï¼ˆæ¨¡å‹é€‰æ‹©ã€åŠŸèƒ½å¼€å…³ï¼‰æ”¾åœ¨ `config.yaml` ä¸­
- ç¯å¢ƒå˜é‡ä¼šè‡ªåŠ¨è¦†ç›– config.yaml ä¸­çš„åŒåé…ç½®

#### 1. ç¯å¢ƒå˜é‡é…ç½®ï¼ˆ.env æ–‡ä»¶ï¼‰

åˆ›å»º `.env` æ–‡ä»¶å­˜å‚¨æ•æ„Ÿä¿¡æ¯å’Œè¿æ¥é…ç½®ï¼š

```bash
# API å¯†é’¥å’Œè®¤è¯ä¿¡æ¯ï¼ˆæ•æ„Ÿä¿¡æ¯ï¼‰
MEMOBASE_LLM_API_KEY=your-openai-api-key
MEMOBASE_EMBEDDING_API_KEY=your-embedding-api-key  # å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ LLM_API_KEY

# Lindorm Table (MySQLåè®®) è¿æ¥é…ç½®
MEMOBASE_LINDORM_TABLE_HOST=localhost
MEMOBASE_LINDORM_TABLE_PORT=33060
MEMOBASE_LINDORM_TABLE_USERNAME=root
MEMOBASE_LINDORM_TABLE_PASSWORD=your-table-password
MEMOBASE_LINDORM_TABLE_DATABASE=memobase

# Lindorm Search è¿æ¥é…ç½®
MEMOBASE_LINDORM_SEARCH_HOST=localhost
MEMOBASE_LINDORM_SEARCH_PORT=30070
MEMOBASE_LINDORM_SEARCH_USERNAME=search-username  
MEMOBASE_LINDORM_SEARCH_PASSWORD=search-password  
MEMOBASE_LINDORM_SEARCH_USE_SSL=false

# è‡ªå®šä¹‰ API ç«¯ç‚¹ï¼ˆå¯é€‰ï¼Œç”¨äºç§æœ‰éƒ¨ç½²ï¼‰
MEMOBASE_LLM_BASE_URL=https://your-custom-api.com/v1
MEMOBASE_EMBEDDING_BASE_URL=https://your-embedding-api.com/v1
```

#### 2. é¡¹ç›®é…ç½®æ–‡ä»¶ï¼ˆconfig.yamlï¼‰

åˆ›å»º `config.yaml` æ–‡ä»¶å®šä¹‰åº”ç”¨çº§é…ç½®ï¼š

```yaml
# è¯­è¨€å’Œæœ¬åœ°åŒ–è®¾ç½®
language: zh  # æ”¯æŒ enï¼ˆè‹±æ–‡ï¼‰æˆ– zhï¼ˆä¸­æ–‡ï¼‰
use_timezone: Asia/Shanghai  # æ—¶åŒºè®¾ç½®ï¼šUTC, America/New_York, Europe/London, Asia/Tokyo, Asia/Shanghai

# LLM æ¨¡å‹é€‰æ‹©
best_llm_model: gpt-4o-mini  # ä¸»è¦æ¨¡å‹
thinking_llm_model: o4-mini   # æ€è€ƒé“¾æ¨¡å‹
summary_llm_model: gpt-3.5-turbo  # æ‘˜è¦æ¨¡å‹ï¼ˆå¯é€‰ï¼‰

# ç´¢å¼•åç§°é…ç½®ï¼ˆé€šå¸¸ä¸éœ€è¦ä¿®æ”¹ï¼‰
lindorm_search_events_index: memobase_events
lindorm_search_event_gists_index: memobase_event_gists

# åµŒå…¥é…ç½®
enable_event_embedding: true
embedding_provider: openai  # å¯é€‰ï¼šopenai æˆ– jina
embedding_model: text-embedding-3-small
embedding_dim: 1536
embedding_max_token_size: 8192

# è®°å¿†æå–é…ç½®
max_profile_subtopics: 15  # æ¯ä¸ªä¸»é¢˜çš„æœ€å¤§å­ä¸»é¢˜æ•°
max_chat_blob_buffer_process_token_size: 16384  # å¯¹è¯ç¼“å†²åŒºå¤§å°
minimum_chats_token_size_for_event_summary: 256  # è§¦å‘äº‹ä»¶æ‘˜è¦çš„æœ€å°tokenæ•°

# ç¼“å†²åŒºç®¡ç†é…ç½®
max_chat_blob_buffer_token_size: 8192  # ç¼“å†²åŒºæœ€å¤§tokenæ•°ï¼Œè¾¾åˆ°æ­¤å€¼è‡ªåŠ¨è§¦å‘å¤„ç†

# æ¡£æ¡ˆç®¡ç†è®¾ç½®
profile_strict_mode: false  # ä¸¥æ ¼æ¨¡å¼
profile_validate_mode: true  # éªŒè¯æ¨¡å¼
event_theme_requirement: "Focus on the user's infos, not its instructions"

# é«˜çº§é…ç½®ï¼ˆé€šå¸¸ä¸éœ€è¦ä¿®æ”¹ï¼‰
persistent_chat_blobs: false  # æ˜¯å¦æŒä¹…åŒ–å¯¹è¯æ•°æ®
llm_tab_separator: "::"  # LLM è¾“å‡ºåˆ†éš”ç¬¦
max_pre_profile_token_size: 128  # é¢„å¤„ç†æ¡£æ¡ˆçš„æœ€å¤§tokenæ•°
```

#### 3. å®Œæ•´é…ç½®ç¤ºä¾‹

**é¡¹ç›®ç»“æ„ï¼š**
```
your-project/
â”œâ”€â”€ .env                    # æ•æ„Ÿä¿¡æ¯ï¼ˆåŠ å…¥ .gitignoreï¼‰
â”œâ”€â”€ .env.example           # ç¯å¢ƒå˜é‡æ¨¡æ¿ï¼ˆå¯æäº¤åˆ°ä»£ç åº“ï¼‰
â”œâ”€â”€ config.yaml            # é¡¹ç›®é…ç½®ï¼ˆå¯æäº¤åˆ°ä»£ç åº“ï¼‰
â”œâ”€â”€ config.yaml.example    # é…ç½®æ¨¡æ¿
â””â”€â”€ main.py               # åº”ç”¨ä»£ç 
```

**.env.example**ï¼ˆæ¨¡æ¿æ–‡ä»¶ï¼Œæäº¤åˆ°ä»£ç åº“ï¼‰ï¼š
```bash
# å¤åˆ¶æ­¤æ–‡ä»¶ä¸º .env å¹¶å¡«å…¥å®é™…å€¼

# === API å¯†é’¥é…ç½® ===
MEMOBASE_LLM_API_KEY=your-openai-api-key-here
# MEMOBASE_EMBEDDING_API_KEY=optional-separate-embedding-key

# === Lindorm Table è¿æ¥é…ç½® ===
MEMOBASE_LINDORM_TABLE_HOST=localhost
MEMOBASE_LINDORM_TABLE_PORT=33060
MEMOBASE_LINDORM_TABLE_USERNAME=root
MEMOBASE_LINDORM_TABLE_PASSWORD=your-password-here
MEMOBASE_LINDORM_TABLE_DATABASE=memobase

# === Lindorm Search è¿æ¥é…ç½® ===
MEMOBASE_LINDORM_SEARCH_HOST=localhost
MEMOBASE_LINDORM_SEARCH_PORT=30070
# MEMOBASE_LINDORM_SEARCH_USERNAME=optional-if-auth-enabled
# MEMOBASE_LINDORM_SEARCH_PASSWORD=optional-if-auth-enabled
# MEMOBASE_LINDORM_SEARCH_USE_SSL=false

# === è‡ªå®šä¹‰ç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰ ===
# MEMOBASE_LLM_BASE_URL=https://your-custom-api.com/v1
# MEMOBASE_EMBEDDING_BASE_URL=https://your-embedding-api.com/v1
```

**config.yaml.example**ï¼ˆé…ç½®æ¨¡æ¿ï¼‰ï¼š
```yaml
# === åŸºç¡€é…ç½® ===
language: zh  # é€‰æ‹©è¯­è¨€ï¼šen æˆ– zh
use_timezone: Asia/Shanghai  # æ—¶åŒºè®¾ç½®

# === æ¨¡å‹é…ç½® ===
# æ ¹æ®éœ€æ±‚å’Œæˆæœ¬é€‰æ‹©åˆé€‚çš„æ¨¡å‹
best_llm_model: gpt-4o-mini  # å¯é€‰ï¼šgpt-4o, gpt-3.5-turbo
thinking_llm_model: o4-mini  # æ€è€ƒé“¾æ¨¡å‹
embedding_model: text-embedding-3-small  # å¯é€‰ï¼štext-embedding-3-large

# === åµŒå…¥é…ç½® ===
enable_event_embedding: true
embedding_provider: openai  # æˆ– jina
embedding_dim: 1536

# === è®°å¿†æå–é…ç½® ===
max_profile_subtopics: 15
profile_validate_mode: true

# === ç´¢å¼•åç§°ï¼ˆé€šå¸¸ä¸éœ€è¦ä¿®æ”¹ï¼‰===
lindorm_search_events_index: memobase_events
lindorm_search_event_gists_index: memobase_event_gists
```

#### 4. é…ç½®åŠ è½½é¡ºåº

```python
from lindormmemobase import LindormMemobase

# æ–¹å¼1ï¼šè‡ªåŠ¨åŠ è½½ï¼ˆæ¨èï¼‰
# è‡ªåŠ¨æŒ‰ä»¥ä¸‹é¡ºåºåŠ è½½ï¼š
# 1. è¯»å– config.yamlï¼ˆå¦‚æœå­˜åœ¨ï¼‰
# 2. è¯»å–ç¯å¢ƒå˜é‡ï¼ˆè¦†ç›– config.yaml ä¸­çš„åŒåé¡¹ï¼‰
# 3. ä½¿ç”¨é»˜è®¤å€¼ï¼ˆæœªé…ç½®çš„é¡¹ï¼‰
memobase = LindormMemobase()

# æ–¹å¼2ï¼šæŒ‡å®šé…ç½®æ–‡ä»¶
memobase = LindormMemobase.from_yaml_file("custom-config.yaml")

# æ–¹å¼3ï¼šä»£ç ä¸­è¦†ç›–ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
memobase = LindormMemobase.from_config(
    language="en",  # è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è¯­è¨€è®¾ç½®
    best_llm_model="gpt-4o"  # è¦†ç›–æ¨¡å‹é€‰æ‹©
)
```

#### 5. é…ç½®æœ€ä½³å®è·µ

**å¼€å‘ç¯å¢ƒï¼š**
- ä½¿ç”¨ `.env` æ–‡ä»¶ç®¡ç† API å¯†é’¥
- åœ¨ `config.yaml` ä¸­ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹é™ä½æˆæœ¬
- å¯ç”¨æ‰€æœ‰è°ƒè¯•å’ŒéªŒè¯é€‰é¡¹

**ç”Ÿäº§ç¯å¢ƒï¼š**
- ä½¿ç”¨ç¯å¢ƒå˜é‡æ³¨å…¥æ•æ„Ÿä¿¡æ¯ï¼ˆä¸ä½¿ç”¨ .env æ–‡ä»¶ï¼‰
- åœ¨ `config.yaml` ä¸­é…ç½®æ€§èƒ½ä¼˜åŒ–å‚æ•°
- æ ¹æ®è´Ÿè½½è°ƒæ•´ token é™åˆ¶å’Œç¼“å†²åŒºå¤§å°

**å¤šç¯å¢ƒç®¡ç†ï¼š**
```bash
# å¼€å‘ç¯å¢ƒ
cp config.dev.yaml config.yaml
export MEMOBASE_LLM_API_KEY=$DEV_API_KEY

# æµ‹è¯•ç¯å¢ƒ
cp config.test.yaml config.yaml
export MEMOBASE_LLM_API_KEY=$TEST_API_KEY

# ç”Ÿäº§ç¯å¢ƒ
cp config.prod.yaml config.yaml
export MEMOBASE_LLM_API_KEY=$PROD_API_KEY
```

**Docker éƒ¨ç½²ç¤ºä¾‹ï¼š**
```dockerfile
FROM python:3.9

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

# å¤åˆ¶é…ç½®æ–‡ä»¶ï¼ˆä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰
COPY config.yaml .
COPY . .

# è¿è¡Œæ—¶é€šè¿‡ç¯å¢ƒå˜é‡æ³¨å…¥æ•æ„Ÿä¿¡æ¯
# docker run -e MEMOBASE_LLM_API_KEY=xxx ...
CMD ["python", "main.py"]
```

## æ ¸å¿ƒæ¦‚å¿µ

### 1. Blobï¼ˆæ•°æ®å—ï¼‰

Blob æ˜¯è¾“å…¥æ•°æ®çš„åŸºæœ¬å•ä½ï¼Œæ”¯æŒå¤šç§ç±»å‹ï¼š

- **ChatBlob**: å¯¹è¯æ¶ˆæ¯
- **DocBlob**: æ–‡æ¡£å†…å®¹
- **CodeBlob**: ä»£ç ç‰‡æ®µ
- **ImageBlob**: å›¾åƒæ•°æ®
- **TranscriptBlob**: éŸ³é¢‘è½¬å½•

### 2. Profileï¼ˆç”¨æˆ·æ¡£æ¡ˆï¼‰

ç”¨æˆ·æ¡£æ¡ˆæŒ‰ä¸»é¢˜ï¼ˆtopicï¼‰å’Œå­ä¸»é¢˜ï¼ˆsubtopicï¼‰ç»„ç»‡ï¼š

```python
Profile(
    topic="personal_info",
    subtopics={
        "basic": ProfileEntry(content="å¼ å°æ˜ï¼Œ25å²ï¼Œè½¯ä»¶å·¥ç¨‹å¸ˆ"),
        "location": ProfileEntry(content="åœ¨åŒ—äº¬å·¥ä½œ")
    }
)
```

### 3. Eventï¼ˆäº‹ä»¶ï¼‰

äº‹ä»¶æ˜¯å¸¦æ—¶é—´æˆ³çš„ç”¨æˆ·æ´»åŠ¨è®°å½•ï¼Œæ”¯æŒè¯­ä¹‰æœç´¢ã€‚

### 4. ProfileConfigï¼ˆæ¡£æ¡ˆé…ç½®ï¼‰

æ§åˆ¶è®°å¿†æå–è¡Œä¸ºçš„é…ç½®å¯¹è±¡ã€‚

## API å‚è€ƒ

### åˆå§‹åŒ–

#### æ–¹æ³• 1ï¼šä½¿ç”¨é»˜è®¤é…ç½®

```python
from lindormmemobase import LindormMemobase

# è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡å’Œ config.yaml åŠ è½½é…ç½®
memobase = LindormMemobase()
```

#### æ–¹æ³• 2ï¼šä» YAML æ–‡ä»¶åŠ è½½

```python
# æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„
memobase = LindormMemobase.from_yaml_file("path/to/config.yaml")
```

#### æ–¹æ³• 3ï¼šä½¿ç”¨å‚æ•°åˆå§‹åŒ–

```python
# ç›´æ¥ä¼ å…¥é…ç½®å‚æ•°
memobase = LindormMemobase.from_config(
    language="zh",
    llm_api_key="your-api-key",
    best_llm_model="gpt-4o",
    lindorm_table_host="localhost",
    lindorm_table_port=33060
)
```

#### æ–¹æ³• 4ï¼šä½¿ç”¨ Config å¯¹è±¡

```python
from lindormmemobase import Config

config = Config(
    language="zh",
    llm_api_key="your-api-key",
    lindorm_table_host="localhost"
)
memobase = LindormMemobase(config)
```

### å†…å­˜æå–

ä»ç”¨æˆ·å¯¹è¯ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯å¹¶æ›´æ–°æ¡£æ¡ˆï¼š

```python
async def extract_memories(
    user_id: str,
    blobs: List[Blob],
    profile_config: Optional[ProfileConfig] = None
) -> Dict
```

**å‚æ•°è¯´æ˜ï¼š**
- `user_id`: ç”¨æˆ·å”¯ä¸€æ ‡è¯†ç¬¦
- `blobs`: è¦å¤„ç†çš„æ•°æ®å—åˆ—è¡¨
- `profile_config`: æ¡£æ¡ˆé…ç½®ï¼ˆå¯é€‰ï¼‰

**ç¤ºä¾‹ï¼š**

```python
from lindormmemobase.models.blob import ChatBlob, OpenAICompatibleMessage

# åˆ›å»ºå¯¹è¯æ•°æ®
conversation = ChatBlob(
    messages=[
        OpenAICompatibleMessage(
            role="user", 
            content="æˆ‘å–œæ¬¢æ‰“ç¯®çƒå’Œæ¸¸æ³³"
        ),
        OpenAICompatibleMessage(
            role="assistant", 
            content="å¾ˆå¥½çš„è¿åŠ¨çˆ±å¥½ï¼"
        )
    ]
)

# æå–è®°å¿†
result = await memobase.extract_memories(
    user_id="user123",
    blobs=[conversation]
)
```

### ç”¨æˆ·æ¡£æ¡ˆç®¡ç†

#### è·å–ç”¨æˆ·æ¡£æ¡ˆ

```python
async def get_user_profiles(
    user_id: str,
    topics: Optional[List[str]] = None
) -> List[Profile]
```

**å‚æ•°è¯´æ˜ï¼š**
- `user_id`: ç”¨æˆ·æ ‡è¯†ç¬¦
- `topics`: è¦ç­›é€‰çš„ä¸»é¢˜åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

**ç¤ºä¾‹ï¼š**

```python
# è·å–æ‰€æœ‰æ¡£æ¡ˆ
profiles = await memobase.get_user_profiles("user123")

# åªè·å–ç‰¹å®šä¸»é¢˜
profiles = await memobase.get_user_profiles(
    "user123", 
    topics=["interests", "preferences"]
)

# éå†æ¡£æ¡ˆ
for profile in profiles:
    print(f"ä¸»é¢˜: {profile.topic}")
    for subtopic, entry in profile.subtopics.items():
        print(f"  {subtopic}: {entry.content}")
```

#### è·å–ç›¸å…³æ¡£æ¡ˆ

æ ¹æ®å½“å‰å¯¹è¯å†…å®¹æ™ºèƒ½ç­›é€‰ç›¸å…³æ¡£æ¡ˆï¼š

```python
async def get_relevant_profiles(
    user_id: str,
    conversation: List[OpenAICompatibleMessage],
    topics: Optional[List[str]] = None,
    max_profiles: int = 10,
    max_profile_token_size: int = 4000
) -> List[Profile]
```

**ç¤ºä¾‹ï¼š**

```python
conversation = [
    OpenAICompatibleMessage(
        role="user", 
        content="æˆ‘æƒ³è§„åˆ’ä¸‹ä¸ªæœˆçš„æ—…è¡Œ"
    )
]

relevant_profiles = await memobase.get_relevant_profiles(
    user_id="user123",
    conversation=conversation,
    topics=["travel", "preferences"],
    max_profiles=5
)
```

#### æœç´¢æ¡£æ¡ˆ

åŸºäºæ–‡æœ¬æŸ¥è¯¢æœç´¢æ¡£æ¡ˆï¼š

```python
async def search_profiles(
    user_id: str,
    query: str,
    topics: Optional[List[str]] = None,
    max_results: int = 10
) -> List[Profile]
```

**ç¤ºä¾‹ï¼š**

```python
profiles = await memobase.search_profiles(
    user_id="user123",
    query="æœ€å–œæ¬¢çš„é¤å…",
    topics=["food", "dining"],
    max_results=5
)
```

### äº‹ä»¶ç®¡ç†

#### è·å–æœ€è¿‘äº‹ä»¶

```python
async def get_events(
    user_id: str,
    time_range_in_days: int = 21,
    limit: int = 100
) -> List[dict]
```

**è¿”å›æ ¼å¼ï¼š**
```python
{
    "id": "event_id",
    "content": "äº‹ä»¶å†…å®¹",
    "created_at": "2024-01-01T12:00:00",
    "updated_at": "2024-01-01T12:00:00"
}
```

**ç¤ºä¾‹ï¼š**

```python
# è·å–æœ€è¿‘ 30 å¤©çš„äº‹ä»¶
events = await memobase.get_events(
    user_id="user123",
    time_range_in_days=30,
    limit=50
)

for event in events:
    print(f"{event['created_at']}: {event['content']}")
```

#### æœç´¢äº‹ä»¶

ä½¿ç”¨è¯­ä¹‰æœç´¢æŸ¥æ‰¾ç›¸å…³äº‹ä»¶ï¼š

```python
async def search_events(
    user_id: str,
    query: str,
    limit: int = 10,
    similarity_threshold: float = 0.2,
    time_range_in_days: int = 21
) -> List[dict]
```

**å‚æ•°è¯´æ˜ï¼š**
- `query`: æœç´¢æŸ¥è¯¢æ–‡æœ¬
- `similarity_threshold`: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰
- `time_range_in_days`: æœç´¢æ—¶é—´èŒƒå›´

**ç¤ºä¾‹ï¼š**

```python
# æœç´¢ä¸"é¡¹ç›®è¿›åº¦"ç›¸å…³çš„äº‹ä»¶
events = await memobase.search_events(
    user_id="user123",
    query="é¡¹ç›®è¿›åº¦ä¼šè®®",
    limit=5,
    similarity_threshold=0.3,
    time_range_in_days=7
)

for event in events:
    similarity = event['similarity']
    print(f"ç›¸ä¼¼åº¦ {similarity:.2f}: {event['content']}")
```

### ä¸Šä¸‹æ–‡ç”Ÿæˆ

ä¸ºå¯¹è¯ç”ŸæˆåŒ…å«ç”¨æˆ·æ¡£æ¡ˆå’Œç›¸å…³äº‹ä»¶çš„ä¸Šä¸‹æ–‡ï¼š

```python
async def get_conversation_context(
    user_id: str,
    conversation: List[OpenAICompatibleMessage],
    profile_config: Optional[ProfileConfig] = None,
    max_token_size: int = 2000,
    prefer_topics: Optional[List[str]] = None,
    time_range_in_days: int = 30,
    event_similarity_threshold: float = 0.2,
    profile_event_ratio: float = 0.6,
    require_event_summary: bool = False,
    customize_context_prompt: Optional[str] = None
) -> str
```

**é‡è¦å‚æ•°ï¼š**
- `max_token_size`: ä¸Šä¸‹æ–‡æœ€å¤§ token æ•°
- `prefer_topics`: ä¼˜å…ˆé€‰æ‹©çš„ä¸»é¢˜
- `profile_event_ratio`: æ¡£æ¡ˆä¸äº‹ä»¶å†…å®¹çš„æ¯”ä¾‹ï¼ˆ0.6 è¡¨ç¤º 60% æ¡£æ¡ˆï¼Œ40% äº‹ä»¶ï¼‰
- `require_event_summary`: æ˜¯å¦åŒ…å«äº‹ä»¶æ‘˜è¦
- `customize_context_prompt`: è‡ªå®šä¹‰ä¸Šä¸‹æ–‡ç”Ÿæˆæç¤º

**ç¤ºä¾‹ï¼š**

```python
conversation = [
    OpenAICompatibleMessage(
        role="user", 
        content="ä»Šå¤©æ™šä¸Šåƒä»€ä¹ˆå¥½ï¼Ÿ"
    )
]

context = await memobase.get_conversation_context(
    user_id="user123",
    conversation=conversation,
    prefer_topics=["dietary_preferences", "favorite_foods"],
    max_token_size=1500,
    profile_event_ratio=0.7,  # 70% æ¡£æ¡ˆï¼Œ30% äº‹ä»¶
    time_range_in_days=14
)

print(f"ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ï¼š\n{context}")
```

### ç¼“å†²åŒºç®¡ç†

LindormMemobase æä¾›æ™ºèƒ½çš„ç¼“å†²åŒºç®¡ç†åŠŸèƒ½ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ”¶é›†å’Œæ‰¹é‡å¤„ç†å¯¹è¯æ•°æ®ï¼Œæé«˜è®°å¿†æå–çš„æ•ˆç‡ã€‚

#### æ ¸å¿ƒæ¦‚å¿µ

- **ç¼“å†²åŒº**: ä¸´æ—¶å­˜å‚¨å¾…å¤„ç†çš„å¯¹è¯æ•°æ®
- **æ‰¹é‡å¤„ç†**: å½“ç¼“å†²åŒºè¾¾åˆ°ä¸€å®šå®¹é‡æ—¶è‡ªåŠ¨è§¦å‘å¤„ç†
- **çŠ¶æ€ç®¡ç†**: è·Ÿè¸ªæ¯ä¸ªæ•°æ®å—çš„å¤„ç†çŠ¶æ€ï¼ˆidleã€processingã€doneã€failedï¼‰
- **æ™ºèƒ½è°ƒåº¦**: æ ¹æ®tokenå¤§å°å’Œæ•°æ®é‡æ™ºèƒ½å†³å®šå¤„ç†æ—¶æœº

#### æ·»åŠ æ•°æ®åˆ°ç¼“å†²åŒº

```python
async def add_blob_to_buffer(
    user_id: str,
    blob: Blob,
    blob_id: Optional[str] = None
) -> str
```

**å‚æ•°è¯´æ˜ï¼š**
- `user_id`: ç”¨æˆ·å”¯ä¸€æ ‡è¯†ç¬¦
- `blob`: è¦æ·»åŠ çš„æ•°æ®å—ï¼ˆChatBlobã€DocBlobç­‰ï¼‰
- `blob_id`: å¯é€‰çš„è‡ªå®šä¹‰IDï¼Œé»˜è®¤ç”ŸæˆUUID

**ç¤ºä¾‹ï¼š**

```python
from lindormmemobase.models.blob import ChatBlob, BlobType, OpenAICompatibleMessage

# åˆ›å»ºèŠå¤©æ•°æ®å—
chat_blob = ChatBlob(
    messages=[
        OpenAICompatibleMessage(role="user", content="æˆ‘å–œæ¬¢å–å’–å•¡"),
        OpenAICompatibleMessage(role="assistant", content="å’–å•¡æ˜¯å¾ˆå¥½çš„é€‰æ‹©ï¼")
    ],
    type=BlobType.chat
)

# æ·»åŠ åˆ°ç¼“å†²åŒº
blob_id = await memobase.add_blob_to_buffer("user123", chat_blob)
print(f"å·²æ·»åŠ åˆ°ç¼“å†²åŒº: {blob_id}")
```

#### æ£€æµ‹ç¼“å†²åŒºçŠ¶æ€

```python
async def detect_buffer_full_or_not(
    user_id: str,
    blob_type: BlobType = BlobType.chat
) -> Dict[str, Any]
```

**è¿”å›æ ¼å¼ï¼š**
```python
{
    "is_full": True,  # æ˜¯å¦éœ€è¦å¤„ç†
    "buffer_full_ids": ["blob_id_1", "blob_id_2"],  # éœ€è¦å¤„ç†çš„æ•°æ®å—IDåˆ—è¡¨
    "blob_type": "BlobType.chat"  # æ•°æ®å—ç±»å‹
}
```

**ç¤ºä¾‹ï¼š**

```python
# æ£€æŸ¥ç¼“å†²åŒºçŠ¶æ€
status = await memobase.detect_buffer_full_or_not("user123", BlobType.chat)

print(f"ç¼“å†²åŒºå·²æ»¡: {status['is_full']}")
print(f"å¾…å¤„ç†çš„æ•°æ®å—æ•°é‡: {len(status['buffer_full_ids'])}")

if status["is_full"]:
    print("éœ€è¦å¤„ç†ç¼“å†²åŒºä¸­çš„æ•°æ®")
```

#### å¤„ç†ç¼“å†²åŒºæ•°æ®

```python
async def process_buffer(
    user_id: str,
    blob_type: BlobType = BlobType.chat,
    profile_config: Optional[ProfileConfig] = None,
    blob_ids: Optional[List[str]] = None
) -> Optional[Any]
```

**å‚æ•°è¯´æ˜ï¼š**
- `user_id`: ç”¨æˆ·æ ‡è¯†ç¬¦
- `blob_type`: å¤„ç†çš„æ•°æ®ç±»å‹
- `profile_config`: æ¡£æ¡ˆé…ç½®ï¼ˆå¯é€‰ï¼‰
- `blob_ids`: æŒ‡å®šè¦å¤„ç†çš„æ•°æ®å—IDåˆ—è¡¨ï¼Œä¸ºNoneæ—¶å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ•°æ®

**ç¤ºä¾‹ï¼š**

```python
# å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„èŠå¤©æ•°æ®
result = await memobase.process_buffer("user123", BlobType.chat)
if result:
    print("ç¼“å†²åŒºå¤„ç†å®Œæˆ")

# å¤„ç†ç‰¹å®šçš„æ•°æ®å—
result = await memobase.process_buffer(
    user_id="user123",
    blob_type=BlobType.chat,
    blob_ids=["blob_id_1", "blob_id_2"],
    profile_config=ProfileConfig(language="zh")
)
```

#### è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç¤ºä¾‹

```python
async def smart_chat_processing(user_id: str, user_messages: List[str]):
    """æ™ºèƒ½å¯¹è¯å¤„ç†æµç¨‹ï¼Œé›†æˆç¼“å†²åŒºç®¡ç†"""
    
    for message_content in user_messages:
        # 1. åˆ›å»ºèŠå¤©æ•°æ®å—
        chat_blob = ChatBlob(
            messages=[OpenAICompatibleMessage(role="user", content=message_content)],
            type=BlobType.chat
        )
        
        # 2. æ·»åŠ åˆ°ç¼“å†²åŒº
        blob_id = await memobase.add_blob_to_buffer(user_id, chat_blob)
        print(f"æ¶ˆæ¯å·²ç¼“å†²: {blob_id}")
        
        # 3. æ£€æŸ¥ç¼“å†²åŒºçŠ¶æ€
        status = await memobase.detect_buffer_full_or_not(user_id, BlobType.chat)
        
        # 4. è‡ªåŠ¨å¤„ç†æ»¡è½½çš„ç¼“å†²åŒº
        if status["is_full"]:
            print(f"ç¼“å†²åŒºå·²æ»¡ï¼Œå¼€å§‹å¤„ç† {len(status['buffer_full_ids'])} ä¸ªæ•°æ®å—...")
            
            result = await memobase.process_buffer(
                user_id=user_id,
                blob_type=BlobType.chat,
                blob_ids=status["buffer_full_ids"]
            )
            
            if result:
                print("âœ“ ç¼“å†²åŒºå¤„ç†å®Œæˆï¼Œè®°å¿†å·²æå–")
            else:
                print("âœ— ç¼“å†²åŒºå¤„ç†å¤±è´¥")

# ä½¿ç”¨ç¤ºä¾‹
messages = [
    "æˆ‘æ˜¯æå››ï¼Œåœ¨ä¸Šæµ·å·¥ä½œ",
    "æˆ‘å–œæ¬¢è·‘æ­¥å’Œçœ‹ç”µå½±", 
    "æœ€è¿‘åœ¨å­¦ä¹ Pythonç¼–ç¨‹",
    "å‘¨æœ«è®¡åˆ’å»åšç‰©é¦†"
]

await smart_chat_processing("user456", messages)
```

#### æ‰¹é‡å¯¹è¯å¤„ç†ä¸ç¼“å†²åŒºé›†æˆ

```python
class BufferedConversationProcessor:
    """å¸¦ç¼“å†²åŒºçš„å¯¹è¯å¤„ç†å™¨"""
    
    def __init__(self, memobase: LindormMemobase):
        self.memobase = memobase
        self.pending_messages = {}  # ç”¨æˆ·ID -> æ¶ˆæ¯åˆ—è¡¨
    
    async def add_message(self, user_id: str, role: str, content: str):
        """æ·»åŠ å•æ¡æ¶ˆæ¯åˆ°å¤„ç†é˜Ÿåˆ—"""
        if user_id not in self.pending_messages:
            self.pending_messages[user_id] = []
        
        self.pending_messages[user_id].append(
            OpenAICompatibleMessage(role=role, content=content)
        )
        
        # æ¯ç§¯ç´¯5æ¡æ¶ˆæ¯å°±æ·»åŠ åˆ°ç¼“å†²åŒº
        if len(self.pending_messages[user_id]) >= 5:
            await self._flush_to_buffer(user_id)
    
    async def _flush_to_buffer(self, user_id: str):
        """å°†ç§¯ç´¯çš„æ¶ˆæ¯æ·»åŠ åˆ°ç¼“å†²åŒº"""
        if user_id not in self.pending_messages or not self.pending_messages[user_id]:
            return
        
        # åˆ›å»ºèŠå¤©æ•°æ®å—
        chat_blob = ChatBlob(
            messages=self.pending_messages[user_id],
            type=BlobType.chat
        )
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        blob_id = await self.memobase.add_blob_to_buffer(user_id, chat_blob)
        print(f"å¯¹è¯å—å·²æ·»åŠ åˆ°ç¼“å†²åŒº: {blob_id}")
        
        # æ¸…ç©ºå¾…å¤„ç†æ¶ˆæ¯
        self.pending_messages[user_id] = []
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†ç¼“å†²åŒº
        await self._check_and_process_buffer(user_id)
    
    async def _check_and_process_buffer(self, user_id: str):
        """æ£€æŸ¥å¹¶å¤„ç†ç¼“å†²åŒº"""
        status = await self.memobase.detect_buffer_full_or_not(user_id, BlobType.chat)
        
        if status["is_full"]:
            print(f"å¼€å§‹å¤„ç†ç”¨æˆ· {user_id} çš„ç¼“å†²åŒº...")
            result = await self.memobase.process_buffer(
                user_id=user_id,
                blob_type=BlobType.chat,
                blob_ids=status["buffer_full_ids"]
            )
            
            if result:
                print(f"âœ“ ç”¨æˆ· {user_id} ç¼“å†²åŒºå¤„ç†å®Œæˆ")
                return True
            else:
                print(f"âœ— ç”¨æˆ· {user_id} ç¼“å†²åŒºå¤„ç†å¤±è´¥")
                return False
        return False
    
    async def force_process_all(self, user_id: str):
        """å¼ºåˆ¶å¤„ç†ç”¨æˆ·çš„æ‰€æœ‰æ•°æ®"""
        # å…ˆå°†å¾…å¤„ç†æ¶ˆæ¯åˆ·å…¥ç¼“å†²åŒº
        await self._flush_to_buffer(user_id)
        
        # å¤„ç†æ‰€æœ‰ç¼“å†²åŒºæ•°æ®
        result = await self.memobase.process_buffer(user_id, BlobType.chat)
        return result is not None

# ä½¿ç”¨ç¤ºä¾‹
processor = BufferedConversationProcessor(memobase)

# æ¨¡æ‹Ÿå®æ—¶å¯¹è¯
await processor.add_message("user789", "user", "ä½ å¥½ï¼Œæˆ‘æ˜¯æ–°ç”¨æˆ·")
await processor.add_message("user789", "assistant", "æ¬¢è¿ï¼å¾ˆé«˜å…´è®¤è¯†æ‚¨")
await processor.add_message("user789", "user", "æˆ‘æƒ³äº†è§£ä¸€ä¸‹è¿™ä¸ªç³»ç»Ÿ")
# ... æ›´å¤šæ¶ˆæ¯

# å¼ºåˆ¶å¤„ç†æ‰€æœ‰å¾…å¤„ç†æ•°æ®
await processor.force_process_all("user789")
```

#### ç¼“å†²åŒºé…ç½®ä¼˜åŒ–

**config.yaml é…ç½®ï¼š**

```yaml
# ç¼“å†²åŒºå¤§å°é…ç½®
max_chat_blob_buffer_token_size: 8192  # ç¼“å†²åŒºæœ€å¤§tokenæ•°ï¼Œå»ºè®®æ ¹æ®å®é™…ä½¿ç”¨è°ƒæ•´

# å¤„ç†é™åˆ¶é…ç½®  
max_chat_blob_buffer_process_token_size: 16384  # å•æ¬¡å¤„ç†æœ€å¤§tokenæ•°

# æ ¹æ®ä¸åŒåœºæ™¯è°ƒæ•´ï¼š
# - ä½é¢‘å¯¹è¯åœºæ™¯ï¼šå¯è®¾ç½®è¾ƒå°çš„ç¼“å†²åŒºå¤§å°ï¼ˆå¦‚4096ï¼‰
# - é«˜é¢‘å¯¹è¯åœºæ™¯ï¼šå¯è®¾ç½®è¾ƒå¤§çš„ç¼“å†²åŒºå¤§å°ï¼ˆå¦‚16384ï¼‰
# - å®æ—¶å“åº”åœºæ™¯ï¼šè®¾ç½®è¾ƒå°çš„ç¼“å†²åŒºç¡®ä¿åŠæ—¶å¤„ç†
# - æ‰¹å¤„ç†åœºæ™¯ï¼šè®¾ç½®è¾ƒå¤§çš„ç¼“å†²åŒºæé«˜å¤„ç†æ•ˆç‡
```

#### é”™è¯¯å¤„ç†ä¸ç›‘æ§

```python
async def robust_buffer_processing(user_id: str, messages: List[str]):
    """å¸¦é”™è¯¯å¤„ç†çš„ç¼“å†²åŒºå¤„ç†"""
    
    for i, message in enumerate(messages):
        try:
            # æ·»åŠ æ¶ˆæ¯åˆ°ç¼“å†²åŒº
            chat_blob = ChatBlob(
                messages=[OpenAICompatibleMessage(role="user", content=message)],
                type=BlobType.chat
            )
            
            blob_id = await memobase.add_blob_to_buffer(user_id, chat_blob)
            print(f"[{i+1}/{len(messages)}] æ¶ˆæ¯å·²ç¼“å†²: {blob_id}")
            
            # æ£€æŸ¥ç¼“å†²åŒºçŠ¶æ€
            status = await memobase.detect_buffer_full_or_not(user_id, BlobType.chat)
            
            if status["is_full"]:
                print(f"å¤„ç†ç¼“å†²åŒºä¸­çš„ {len(status['buffer_full_ids'])} ä¸ªæ•°æ®å—...")
                
                # å¤„ç†ç¼“å†²åŒº
                result = await memobase.process_buffer(
                    user_id=user_id,
                    blob_type=BlobType.chat,
                    blob_ids=status["buffer_full_ids"]
                )
                
                if result:
                    print("âœ“ ç¼“å†²åŒºå¤„ç†æˆåŠŸ")
                else:
                    print("âš ï¸ ç¼“å†²åŒºå¤„ç†è¿”å›ç©ºç»“æœ")
                    
        except Exception as e:
            print(f"âœ— å¤„ç†æ¶ˆæ¯ {i+1} æ—¶å‡ºé”™: {e}")
            # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†ä¸‹ä¸€æ¡æ¶ˆæ¯
            continue
    
    # æœ€ç»ˆæ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªå¤„ç†çš„æ•°æ®
    try:
        final_status = await memobase.detect_buffer_full_or_not(user_id, BlobType.chat)
        if final_status["buffer_full_ids"]:
            print("å¤„ç†å‰©ä½™ç¼“å†²åŒºæ•°æ®...")
            await memobase.process_buffer(user_id, BlobType.chat)
    except Exception as e:
        print(f"æœ€ç»ˆå¤„ç†å‡ºé”™: {e}")

# ä½¿ç”¨ç¤ºä¾‹
test_messages = [
    "æˆ‘å«ç‹äº”ï¼Œåœ¨æ·±åœ³å·¥ä½œ",
    "æˆ‘æ˜¯ä¸€åæ•°æ®ç§‘å­¦å®¶",
    "å–œæ¬¢ç ”ç©¶æœºå™¨å­¦ä¹ ç®—æ³•", 
    "ä¸šä½™æ—¶é—´å–œæ¬¢è¸¢è¶³çƒ",
    "æœ€è¿‘åœ¨å…³æ³¨å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•"
]

await robust_buffer_processing("user_wang", test_messages)
```

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æ¡£æ¡ˆé…ç½®

```python
from lindormmemobase.models.profile_topic import ProfileConfig

# åˆ›å»ºè‡ªå®šä¹‰æ¡£æ¡ˆé…ç½®
profile_config = ProfileConfig(
    language="zh",
    overwrite_user_profiles=[
        {
            "topic": "health",
            "subtopics": ["exercise", "diet", "sleep"]
        },
        {
            "topic": "work",
            "subtopics": ["projects", "skills", "goals"]
        }
    ],
    event_tags=[
        {"name": "important", "description": "é‡è¦äº‹ä»¶"},
        {"name": "milestone", "description": "é‡Œç¨‹ç¢‘"}
    ]
)

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æå–è®°å¿†
result = await memobase.extract_memories(
    user_id="user123",
    blobs=conversation_blobs,
    profile_config=profile_config
)
```

### æ‰¹é‡å¤„ç†å¯¹è¯

```python
async def process_conversation_history(user_id: str, messages: List[dict]):
    """æ‰¹é‡å¤„ç†å†å²å¯¹è¯"""
    
    # å°†æ¶ˆæ¯åˆ†ç»„ä¸ºå¯¹è¯å—
    blobs = []
    batch_size = 10  # æ¯ä¸ªå—åŒ…å«10æ¡æ¶ˆæ¯
    
    for i in range(0, len(messages), batch_size):
        batch = messages[i:i+batch_size]
        chat_blob = ChatBlob(
            messages=[
                OpenAICompatibleMessage(
                    role=msg["role"],
                    content=msg["content"],
                    created_at=msg.get("timestamp")
                )
                for msg in batch
            ]
        )
        blobs.append(chat_blob)
    
    # æ‰¹é‡æå–è®°å¿†
    result = await memobase.extract_memories(
        user_id=user_id,
        blobs=blobs
    )
    
    return result
```

### å®æ—¶å¯¹è¯é›†æˆ

```python
class ConversationManager:
    """å¯¹è¯ç®¡ç†å™¨ï¼Œé›†æˆè®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self, memobase: LindormMemobase):
        self.memobase = memobase
        self.conversation_buffer = []
    
    async def process_message(self, user_id: str, role: str, content: str):
        """å¤„ç†å•æ¡æ¶ˆæ¯"""
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        message = OpenAICompatibleMessage(role=role, content=content)
        self.conversation_buffer.append(message)
        
        # æ¯ 5 æ¡æ¶ˆæ¯æå–ä¸€æ¬¡è®°å¿†
        if len(self.conversation_buffer) >= 5:
            blob = ChatBlob(messages=self.conversation_buffer)
            
            # å¼‚æ­¥æå–è®°å¿†
            await self.memobase.extract_memories(
                user_id=user_id,
                blobs=[blob]
            )
            
            # æ¸…ç©ºç¼“å†²åŒº
            self.conversation_buffer = []
    
    async def get_context_for_reply(self, user_id: str, current_message: str):
        """ä¸ºå›å¤ç”Ÿæˆä¸Šä¸‹æ–‡"""
        
        conversation = [
            OpenAICompatibleMessage(role="user", content=current_message)
        ]
        
        # è·å–ç›¸å…³ä¸Šä¸‹æ–‡
        context = await self.memobase.get_conversation_context(
            user_id=user_id,
            conversation=conversation,
            max_token_size=1000
        )
        
        return context
```

### é”™è¯¯å¤„ç†

```python
from lindormmemobase import LindormMemobaseError, ConfigurationError

async def safe_extract_memories(memobase, user_id, blobs):
    """å¸¦é”™è¯¯å¤„ç†çš„è®°å¿†æå–"""
    try:
        result = await memobase.extract_memories(
            user_id=user_id,
            blobs=blobs
        )
        return result
    
    except ConfigurationError as e:
        print(f"é…ç½®é”™è¯¯: {e}")
        # å¤„ç†é…ç½®é—®é¢˜
        return None
    
    except LindormMemobaseError as e:
        print(f"æå–å¤±è´¥: {e}")
        # å¤„ç†æå–é”™è¯¯
        return None
    
    except Exception as e:
        print(f"æœªçŸ¥é”™è¯¯: {e}")
        # å¤„ç†å…¶ä»–é”™è¯¯
        return None
```

## å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šèŠå¤©æœºå™¨äººé›†æˆ

```python
import asyncio
from typing import List
from lindormmemobase import LindormMemobase
from lindormmemobase.models.blob import ChatBlob, BlobType, OpenAICompatibleMessage
from lindormmemobase.models.profile_topic import ProfileConfig

class MemoryEnabledChatbot:
    """å¸¦è®°å¿†åŠŸèƒ½çš„èŠå¤©æœºå™¨äºº"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.memobase = LindormMemobase.from_yaml_file(config_path)
        self.profile_config = ProfileConfig(language="zh")
        self.conversation_history = []
    
    async def initialize_user(self, user_id: str):
        """åˆå§‹åŒ–ç”¨æˆ·æ¡£æ¡ˆ"""
        profiles = await self.memobase.get_user_profiles(user_id)
        if not profiles:
            print(f"æ–°ç”¨æˆ· {user_id}ï¼Œå¼€å§‹å»ºç«‹æ¡£æ¡ˆ")
        else:
            print(f"å·²åŠ è½½ç”¨æˆ· {user_id} çš„ {len(profiles)} ä¸ªæ¡£æ¡ˆä¸»é¢˜")
        return profiles
    
    async def chat(self, user_id: str, user_input: str) -> str:
        """å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå›å¤"""
        
        # 1. è®°å½•ç”¨æˆ·è¾“å…¥
        user_message = OpenAICompatibleMessage(
            role="user",
            content=user_input
        )
        self.conversation_history.append(user_message)
        
        # 2. è·å–ç›¸å…³ä¸Šä¸‹æ–‡
        context = await self.memobase.get_conversation_context(
            user_id=user_id,
            conversation=self.conversation_history[-10:],  # æœ€è¿‘10æ¡æ¶ˆæ¯
            max_token_size=1500,
            prefer_topics=self._infer_topics(user_input),
            time_range_in_days=30
        )
        
        # 3. ç”Ÿæˆå›å¤ï¼ˆè¿™é‡Œæ¨¡æ‹Ÿ LLM è°ƒç”¨ï¼‰
        assistant_reply = await self._generate_reply(context, user_input)
        
        # 4. è®°å½•åŠ©æ‰‹å›å¤
        assistant_message = OpenAICompatibleMessage(
            role="assistant",
            content=assistant_reply
        )
        self.conversation_history.append(assistant_message)
        
        # 5. å¼‚æ­¥æå–è®°å¿†ï¼ˆæ¯2è½®å¯¹è¯æå–ä¸€æ¬¡ï¼‰
        if len(self.conversation_history) % 4 == 0:
            await self._extract_memories_async(user_id)
        
        return assistant_reply
    
    def _infer_topics(self, user_input: str) -> List[str]:
        """æ¨æ–­ç›¸å…³ä¸»é¢˜"""
        topic_keywords = {
            "work": ["å·¥ä½œ", "é¡¹ç›®", "ä¼šè®®", "ä»»åŠ¡"],
            "health": ["å¥åº·", "è¿åŠ¨", "ç¡çœ ", "é¥®é£Ÿ"],
            "interests": ["çˆ±å¥½", "å–œæ¬¢", "å…´è¶£", "å¨±ä¹"],
            "personal_info": ["æˆ‘æ˜¯", "æˆ‘å«", "å¹´é¾„", "ä½åœ¨"]
        }
        
        relevant_topics = []
        for topic, keywords in topic_keywords.items():
            if any(keyword in user_input for keyword in keywords):
                relevant_topics.append(topic)
        
        return relevant_topics if relevant_topics else ["general"]
    
    async def _generate_reply(self, context: str, user_input: str) -> str:
        """ç”Ÿæˆå›å¤ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨ LLM API
        # ç¤ºä¾‹ï¼šresponse = await llm.generate(prompt=f"{context}\nç”¨æˆ·ï¼š{user_input}")
        return f"ç†è§£äº†æ‚¨çš„éœ€æ±‚ã€‚åŸºäºæ‚¨çš„æ¡£æ¡ˆä¿¡æ¯ï¼Œæˆ‘äº†è§£åˆ°ï¼š{context[:100]}..."
    
    async def _extract_memories_async(self, user_id: str):
        """ä½¿ç”¨ç¼“å†²åŒºå¼‚æ­¥æå–è®°å¿†"""
        # åˆ›å»ºå¯¹è¯å—å¹¶æ·»åŠ åˆ°ç¼“å†²åŒº
        blob = ChatBlob(
            messages=self.conversation_history[-4:]  # æœ€è¿‘2è½®å¯¹è¯
        )
        
        try:
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            blob_id = await self.memobase.add_blob_to_buffer(user_id, blob)
            print(f"å¯¹è¯å·²æ·»åŠ åˆ°ç¼“å†²åŒº: {blob_id}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†ç¼“å†²åŒº
            status = await self.memobase.detect_buffer_full_or_not(user_id, BlobType.chat)
            
            if status["is_full"]:
                print(f"ç¼“å†²åŒºå·²æ»¡ï¼Œå¼€å§‹å¤„ç† {len(status['buffer_full_ids'])} ä¸ªæ•°æ®å—...")
                result = await self.memobase.process_buffer(
                    user_id=user_id,
                    blob_type=BlobType.chat,
                    profile_config=self.profile_config,
                    blob_ids=status["buffer_full_ids"]
                )
                
                if result:
                    print("âœ“ ç¼“å†²åŒºå¤„ç†å®Œæˆï¼Œè®°å¿†å·²æå–")
                else:
                    print("âš ï¸ ç¼“å†²åŒºå¤„ç†è¿”å›ç©ºç»“æœ")
                    
        except Exception as e:
            print(f"ç¼“å†²åŒºå¤„ç†å¤±è´¥: {e}")
    
    async def force_extract_all_memories(self, user_id: str):
        """å¼ºåˆ¶å¤„ç†æ‰€æœ‰ç¼“å†²åŒºæ•°æ®"""
        try:
            result = await self.memobase.process_buffer(user_id, BlobType.chat)
            if result:
                print("âœ“ æ‰€æœ‰ç¼“å†²åŒºæ•°æ®å·²å¤„ç†å®Œæˆ")
                return True
            else:
                print("âš ï¸ æ²¡æœ‰å¾…å¤„ç†çš„ç¼“å†²åŒºæ•°æ®")
                return False
        except Exception as e:
            print(f"å¼ºåˆ¶å¤„ç†ç¼“å†²åŒºå¤±è´¥: {e}")
            return False
    
    async def get_user_summary(self, user_id: str) -> str:
        """è·å–ç”¨æˆ·æ¡£æ¡ˆæ‘˜è¦"""
        profiles = await self.memobase.get_user_profiles(user_id)
        
        summary = []
        for profile in profiles:
            summary.append(f"ã€{profile.topic}ã€‘")
            for subtopic, entry in profile.subtopics.items():
                summary.append(f"  - {subtopic}: {entry.content}")
        
        return "\n".join(summary)

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    # åˆå§‹åŒ–èŠå¤©æœºå™¨äºº
    chatbot = MemoryEnabledChatbot()
    user_id = "test_user_001"
    
    # åˆå§‹åŒ–ç”¨æˆ·
    await chatbot.initialize_user(user_id)
    
    # æ¨¡æ‹Ÿå¯¹è¯
    conversations = [
        "ä½ å¥½ï¼Œæˆ‘æ˜¯å°æ˜ï¼Œä»Šå¹´28å²ï¼Œæ˜¯ä¸€åäº§å“ç»ç†",
        "æˆ‘å–œæ¬¢çœ‹ç§‘å¹»ç”µå½±å’Œæ‰“ç¾½æ¯›çƒ",
        "æœ€è¿‘å·¥ä½œå‹åŠ›æœ‰ç‚¹å¤§ï¼Œç»å¸¸åŠ ç­åˆ°å¾ˆæ™š",
        "å‘¨æœ«æƒ³å»çˆ¬å±±æ”¾æ¾ä¸€ä¸‹"
    ]
    
    for user_input in conversations:
        print(f"\nç”¨æˆ·: {user_input}")
        reply = await chatbot.chat(user_id, user_input)
        print(f"åŠ©æ‰‹: {reply}")
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿå¯¹è¯é—´éš”
    
    # å¼ºåˆ¶å¤„ç†æ‰€æœ‰å‰©ä½™ç¼“å†²åŒºæ•°æ®
    print("\n=== å¤„ç†å‰©ä½™ç¼“å†²åŒºæ•°æ® ===")
    await chatbot.force_extract_all_memories(user_id)
    
    # æŸ¥çœ‹ç”¨æˆ·æ¡£æ¡ˆ
    print("\n=== ç”¨æˆ·æ¡£æ¡ˆæ‘˜è¦ ===")
    summary = await chatbot.get_user_summary(user_id)
    print(summary)
    
    # æœç´¢ç›¸å…³è®°å¿†
    print("\n=== æœç´¢ç›¸å…³è®°å¿† ===")
    events = await chatbot.memobase.search_events(
        user_id=user_id,
        query="è¿åŠ¨çˆ±å¥½",
        limit=3
    )
    for event in events:
        print(f"- {event['content']}")
    
    # æ¼”ç¤ºç¼“å†²åŒºçŠ¶æ€æ£€æŸ¥
    print("\n=== ç¼“å†²åŒºçŠ¶æ€æ£€æŸ¥ ===")
    status = await chatbot.memobase.detect_buffer_full_or_not(user_id, BlobType.chat)
    print(f"ç¼“å†²åŒºå·²æ»¡: {status['is_full']}")
    print(f"å¾…å¤„ç†æ•°æ®å—æ•°é‡: {len(status['buffer_full_ids'])}")

if __name__ == "__main__":
    asyncio.run(main())
```

### ç¤ºä¾‹ 2ï¼šç¼“å†²åŒºç®¡ç†ä¸“ç”¨ç¤ºä¾‹

```python
import asyncio
from typing import List
from lindormmemobase import LindormMemobase
from lindormmemobase.models.blob import ChatBlob, BlobType, OpenAICompatibleMessage

async def buffer_management_demo():
    """å®Œæ•´çš„ç¼“å†²åŒºç®¡ç†æ¼”ç¤º"""
    
    # åˆå§‹åŒ–
    memobase = LindormMemobase()
    user_id = "buffer_demo_user"
    
    print("=== ç¼“å†²åŒºç®¡ç†æ¼”ç¤º ===\n")
    
    # 1. å‡†å¤‡æµ‹è¯•å¯¹è¯æ•°æ®
    conversations = [
        ["user", "æˆ‘æ˜¯å¼ ä¸‰ï¼Œåœ¨åŒ—äº¬ä»äº‹AIç ”å‘å·¥ä½œ"],
        ["assistant", "æ‚¨å¥½å¼ ä¸‰ï¼AIç ”å‘æ˜¯å¾ˆæœ‰å‰æ™¯çš„é¢†åŸŸã€‚"],
        ["user", "æˆ‘å¹³æ—¶å–œæ¬¢é˜…è¯»æŠ€æœ¯ä¹¦ç±å’Œè·‘æ­¥"],
        ["assistant", "é˜…è¯»å’Œè·‘æ­¥éƒ½æ˜¯å¾ˆå¥½çš„ä¹ æƒ¯ï¼"],
        ["user", "æœ€è¿‘åœ¨ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨"],
        ["assistant", "LLMç¡®å®æ˜¯å½“å‰çš„çƒ­ç‚¹æŠ€æœ¯ã€‚"],
        ["user", "æˆ‘å¸Œæœ›èƒ½åœ¨è¿™ä¸ªé¢†åŸŸæœ‰æ‰€çªç ´"],
        ["assistant", "ç›¸ä¿¡æ‚¨ä¸€å®šå¯ä»¥çš„ï¼"],
        ["user", "å‘¨æœ«è®¡åˆ’å»å›¾ä¹¦é¦†å­¦ä¹ æ–°æŠ€æœ¯"],
        ["assistant", "å……å®çš„å‘¨æœ«å®‰æ’ï¼"]
    ]
    
    # 2. æ‰¹é‡æ·»åŠ å¯¹è¯åˆ°ç¼“å†²åŒº
    print("1. æ‰¹é‡æ·»åŠ å¯¹è¯åˆ°ç¼“å†²åŒº...")
    blob_ids = []
    
    for i in range(0, len(conversations), 2):  # æ¯2æ¡æ¶ˆæ¯ä¸€ä¸ªå¯¹è¯å—
        if i + 1 < len(conversations):
            # åˆ›å»ºå¯¹è¯å—
            chat_blob = ChatBlob(
                messages=[
                    OpenAICompatibleMessage(role=conversations[i][0], content=conversations[i][1]),
                    OpenAICompatibleMessage(role=conversations[i+1][0], content=conversations[i+1][1])
                ],
                type=BlobType.chat
            )
            
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            blob_id = await memobase.add_blob_to_buffer(user_id, chat_blob)
            blob_ids.append(blob_id)
            print(f"   âœ“ å¯¹è¯å— {len(blob_ids)} å·²æ·»åŠ : {blob_id}")
            
            # æ¯æ·»åŠ ä¸€ä¸ªå¯¹è¯å—å°±æ£€æŸ¥ç¼“å†²åŒºçŠ¶æ€
            status = await memobase.detect_buffer_full_or_not(user_id, BlobType.chat)
            print(f"   - ç¼“å†²åŒºçŠ¶æ€: {'å·²æ»¡' if status['is_full'] else 'æœªæ»¡'} "
                  f"(å¾…å¤„ç†: {len(status['buffer_full_ids'])} ä¸ª)")
            
            if status["is_full"]:
                print(f"   ğŸ”„ ç¼“å†²åŒºå·²æ»¡ï¼Œè‡ªåŠ¨å¤„ç† {len(status['buffer_full_ids'])} ä¸ªæ•°æ®å—...")
                result = await memobase.process_buffer(
                    user_id=user_id,
                    blob_type=BlobType.chat,
                    blob_ids=status["buffer_full_ids"]
                )
                
                if result:
                    print(f"   âœ… ç¼“å†²åŒºå¤„ç†å®Œæˆ")
                else:
                    print(f"   âš ï¸ ç¼“å†²åŒºå¤„ç†è¿”å›ç©ºç»“æœ")
            
            print()  # ç©ºè¡Œåˆ†éš”
    
    # 3. å¤„ç†å‰©ä½™çš„ç¼“å†²åŒºæ•°æ®
    print("2. æ£€æŸ¥å¹¶å¤„ç†å‰©ä½™ç¼“å†²åŒºæ•°æ®...")
    final_status = await memobase.detect_buffer_full_or_not(user_id, BlobType.chat)
    
    if final_status["buffer_full_ids"]:
        print(f"   å‘ç° {len(final_status['buffer_full_ids'])} ä¸ªæœªå¤„ç†çš„æ•°æ®å—")
        result = await memobase.process_buffer(user_id, BlobType.chat)
        if result:
            print("   âœ… å‰©ä½™æ•°æ®å¤„ç†å®Œæˆ")
    else:
        print("   â„¹ï¸ æ²¡æœ‰å‰©ä½™çš„æœªå¤„ç†æ•°æ®")
    
    # 4. éªŒè¯å¤„ç†ç»“æœ
    print("\n3. éªŒè¯å¤„ç†ç»“æœ...")
    
    # è·å–ç”¨æˆ·æ¡£æ¡ˆ
    profiles = await memobase.get_user_profiles(user_id)
    print(f"   ç”Ÿæˆç”¨æˆ·æ¡£æ¡ˆ: {len(profiles)} ä¸ªä¸»é¢˜")
    
    for profile in profiles:
        print(f"   ğŸ“ ä¸»é¢˜: {profile.topic}")
        for subtopic, entry in profile.subtopics.items():
            print(f"      â””â”€â”€ {subtopic}: {entry.content}")
    
    # è·å–äº‹ä»¶
    events = await memobase.get_events(user_id, time_range_in_days=7, limit=10)
    print(f"\n   ç”Ÿæˆäº‹ä»¶è®°å½•: {len(events)} æ¡")
    for event in events[:3]:  # åªæ˜¾ç¤ºå‰3æ¡
        print(f"   ğŸ“… {event['content']}")
    
    # 5. æ¼”ç¤ºæœç´¢åŠŸèƒ½
    print("\n4. æœç´¢ç›¸å…³è®°å¿†...")
    search_results = await memobase.search_events(
        user_id=user_id,
        query="æŠ€æœ¯å­¦ä¹ ",
        limit=3,
        similarity_threshold=0.1
    )
    
    print(f"   æ‰¾åˆ° {len(search_results)} æ¡ç›¸å…³è®°å½•:")
    for result in search_results:
        similarity = result.get('similarity', 0)
        print(f"   ğŸ” (ç›¸ä¼¼åº¦: {similarity:.2f}) {result['content']}")
    
    print(f"\nâœ¨ ç¼“å†²åŒºç®¡ç†æ¼”ç¤ºå®Œæˆï¼ç”¨æˆ· {user_id} çš„è®°å¿†ç³»ç»Ÿå·²å»ºç«‹")

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    asyncio.run(buffer_management_demo())
```

### ç¤ºä¾‹ 3ï¼šæ‰¹é‡æ•°æ®å¯¼å…¥

```python
import asyncio
import json
from datetime import datetime, timedelta
from lindormmemobase import LindormMemobase
from lindormmemobase.models.blob import ChatBlob, OpenAICompatibleMessage

async def import_chat_history(file_path: str, user_id: str):
    """ä»æ–‡ä»¶å¯¼å…¥èŠå¤©å†å²"""
    
    # åˆå§‹åŒ–
    memobase = LindormMemobase()
    
    # è¯»å–èŠå¤©å†å²
    with open(file_path, 'r', encoding='utf-8') as f:
        chat_history = json.load(f)
    
    # æŒ‰æ—¥æœŸåˆ†ç»„å¯¹è¯
    daily_conversations = {}
    for message in chat_history:
        date = message['timestamp'][:10]  # æå–æ—¥æœŸéƒ¨åˆ†
        if date not in daily_conversations:
            daily_conversations[date] = []
        daily_conversations[date].append(message)
    
    # æ‰¹é‡å¤„ç†æ¯å¤©çš„å¯¹è¯
    for date, messages in daily_conversations.items():
        print(f"å¤„ç† {date} çš„å¯¹è¯ ({len(messages)} æ¡æ¶ˆæ¯)")
        
        # åˆ›å»ºå¯¹è¯å—
        blob = ChatBlob(
            messages=[
                OpenAICompatibleMessage(
                    role=msg['role'],
                    content=msg['content'],
                    created_at=msg['timestamp']
                )
                for msg in messages
            ]
        )
        
        # æå–è®°å¿†
        try:
            result = await memobase.extract_memories(
                user_id=user_id,
                blobs=[blob]
            )
            print(f"  âœ“ æˆåŠŸæå– {date} çš„è®°å¿†")
        except Exception as e:
            print(f"  âœ— å¤„ç† {date} å¤±è´¥: {e}")
        
        # é¿å…è¿‡å¿«è°ƒç”¨
        await asyncio.sleep(1)
    
    # ç”Ÿæˆç”¨æˆ·æ¡£æ¡ˆæŠ¥å‘Š
    profiles = await memobase.get_user_profiles(user_id)
    print(f"\nç”¨æˆ·æ¡£æ¡ˆå·²æ›´æ–°ï¼Œå…± {len(profiles)} ä¸ªä¸»é¢˜")
    
    return profiles

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_history = [
        {
            "role": "user",
            "content": "æˆ‘æ˜¯å¼ ä¸‰ï¼Œåœ¨åŒ—äº¬å·¥ä½œ",
            "timestamp": "2024-01-01T10:00:00"
        },
        {
            "role": "assistant",
            "content": "ä½ å¥½å¼ ä¸‰ï¼",
            "timestamp": "2024-01-01T10:00:30"
        },
        {
            "role": "user",
            "content": "æˆ‘å–œæ¬¢ç¼–ç¨‹å’Œé˜…è¯»",
            "timestamp": "2024-01-01T10:01:00"
        }
    ]
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open("chat_history.json", "w", encoding="utf-8") as f:
        json.dump(test_history, f, ensure_ascii=False, indent=2)
    
    # å¯¼å…¥å†å²
    profiles = await import_chat_history("chat_history.json", "user_zhang")
    
    # æ˜¾ç¤ºç»“æœ
    for profile in profiles:
        print(f"\nä¸»é¢˜: {profile.topic}")
        for subtopic, entry in profile.subtopics.items():
            print(f"  {subtopic}: {entry.content}")

if __name__ == "__main__":
    asyncio.run(main())
```
